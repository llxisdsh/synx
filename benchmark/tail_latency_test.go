package benchmark

import (
	"fmt"
	"runtime"
	"slices"
	"sync"
	"sync/atomic"
	"testing"
	"time"

	"github.com/llxisdsh/synx"
	"github.com/puzpuzpuz/xsync/v4"
)

// ============================================================================
// Global Configuration
// ============================================================================

// useBuiltInHasher controls whether to use synx.WithBuiltInHasher[int]()
// Set to true for faster int hashing, false for default behavior
var useBuiltInHasher = false

// Test parameters - adjust for stability vs speed tradeoff
const (
	defaultOpsPerWorker = 50000 // Operations per worker
	defaultKeys         = 10000 // Number of keys
	warmupRounds        = 3     // Warmup iterations before measurement
	measureRounds       = 5     // Measurement rounds to average
	batchSize           = 1     // measure latency per batch to overcome Windows timer precision (~15ms)
)

// Helper to create synx maps with optional built-in hasher
func newFlatMap() *synx.FlatMap[int, int] {
	//goland:noinspection GoBoolExpressions
	if useBuiltInHasher {
		return synx.NewFlatMap[int, int](synx.WithBuiltInHasher[int]())
	}
	return synx.NewFlatMap[int, int]()
}

func newMap() *synx.Map[int, int] {
	//goland:noinspection GoBoolExpressions
	if useBuiltInHasher {
		return synx.NewMap[int, int](synx.WithBuiltInHasher[int]())
	}
	return synx.NewMap[int, int]()
}

// ============================================================================
// Map Adapters
// ============================================================================

type MapInterface interface {
	Store(key, value int)
	Load(key int) (int, bool)
}

type flatMapAdapter struct{ m *synx.FlatMap[int, int] }

func (a *flatMapAdapter) Store(k, v int)         { a.m.Store(k, v) }
func (a *flatMapAdapter) Load(k int) (int, bool) { return a.m.Load(k) }

type mapAdapter struct{ m *synx.Map[int, int] }

func (a *mapAdapter) Store(k, v int)         { a.m.Store(k, v) }
func (a *mapAdapter) Load(k int) (int, bool) { return a.m.Load(k) }

type xsyncMapAdapter struct{ m *xsync.Map[int, int] }

func (a *xsyncMapAdapter) Store(k, v int)         { a.m.Store(k, v) }
func (a *xsyncMapAdapter) Load(k int) (int, bool) { return a.m.Load(k) }

type syncMapAdapter struct{ m *sync.Map }

func (a *syncMapAdapter) Store(k, v int) { a.m.Store(k, v) }
func (a *syncMapAdapter) Load(k int) (int, bool) {
	v, ok := a.m.Load(k)
	if ok {
		return v.(int), true
	}
	return 0, false
}

type rwShardedMapAdapter struct{ m *RWLockShardedMap[int, int] }

func (a *rwShardedMapAdapter) Store(k, v int)         { a.m.Store(k, v) }
func (a *rwShardedMapAdapter) Load(k int) (int, bool) { return a.m.Load(k) }

// ============================================================================
// Latency Result
// ============================================================================

type latencyResult struct {
	name       string
	throughput float64
	avg        time.Duration
	p50        time.Duration
	p99        time.Duration
	p999       time.Duration
	max        time.Duration
	slowRate   float64 // % of ops > 1ms
}

// ms formats duration as milliseconds with 2 decimal places
func ms(d time.Duration) string {
	return fmt.Sprintf("%.2fµs", float64(d.Nanoseconds())/float64(time.Microsecond))
}

func runLatencyTest(workers, keys, opsPerWorker int, m MapInterface) latencyResult {
	batches := opsPerWorker / batchSize
	if batches < 1 {
		batches = 1
	}
	totalBatches := workers * batches
	samples := make([]int64, totalBatches)
	var sampleIdx atomic.Int64
	var slowOps atomic.Int64

	// Pre-populate
	for i := range keys {
		m.Store(i, i)
	}
	runtime.GC()

	var wg sync.WaitGroup
	start := time.Now()

	for w := range workers {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()
			baseKey := workerID * opsPerWorker

			for b := range batches {
				batchStart := time.Now()

				// Run a batch of operations
				for i := range batchSize {
					key := (baseKey + b*batchSize + i) % keys
					if i%5 == 0 {
						m.Store(key, workerID)
					} else {
						_, _ = m.Load(key)
					}
				}

				batchLatency := time.Since(batchStart).Nanoseconds()
				perOpLatency := batchLatency / int64(batchSize) // Average per operation

				if perOpLatency > int64(time.Millisecond) {
					slowOps.Add(1)
				}

				idx := sampleIdx.Add(1) - 1
				if idx < int64(totalBatches) {
					samples[idx] = perOpLatency
				}
			}
		}(w)
	}
	wg.Wait()
	elapsed := time.Since(start)

	slices.Sort(samples)

	var sum int64
	for _, v := range samples {
		sum += v
	}

	totalOps := workers * batches * batchSize

	return latencyResult{
		throughput: float64(totalOps) / elapsed.Seconds(),
		avg:        time.Duration(sum / int64(len(samples))),
		p50:        time.Duration(samples[len(samples)/2]),
		p99:        time.Duration(samples[int(float64(len(samples))*0.99)]),
		p999:       time.Duration(samples[int(float64(len(samples)-1)*0.999)]),
		max:        time.Duration(samples[len(samples)-1]),
		slowRate:   float64(slowOps.Load()) / float64(totalOps) * 100,
	}
}

// runWithWarmup runs warmup rounds then measurement rounds and returns averaged result
func runWithWarmup(workers, keys, ops int, makeMap func() MapInterface) latencyResult {
	// Warmup
	for range warmupRounds {
		m := makeMap()
		_ = runLatencyTest(workers, keys, ops/10, m) // Smaller warmup
	}
	runtime.GC()
	time.Sleep(10 * time.Millisecond)

	// Measurement rounds
	var results []latencyResult
	for range measureRounds {
		m := makeMap()
		r := runLatencyTest(workers, keys, ops, m)
		results = append(results, r)
		runtime.GC()
	}

	// Average results
	var avgResult latencyResult
	for _, r := range results {
		avgResult.throughput += r.throughput
		avgResult.avg += r.avg
		avgResult.p50 += r.p50
		avgResult.p99 += r.p99
		avgResult.p999 += r.p999
		avgResult.max += r.max
		avgResult.slowRate += r.slowRate
	}
	n := float64(len(results))
	avgResult.throughput /= n
	avgResult.avg /= time.Duration(n)
	avgResult.p50 /= time.Duration(n)
	avgResult.p99 /= time.Duration(n)
	avgResult.p999 /= time.Duration(n)
	avgResult.max /= time.Duration(n)
	avgResult.slowRate /= n

	return avgResult
}

// ============================================================================
// Main Test
// ============================================================================

func TestLatencySummary(t *testing.T) {
	numCPU := runtime.GOMAXPROCS(0)
	workers := numCPU * 2
	keys := defaultKeys
	ops := defaultOpsPerWorker

	t.Logf("=== Tail Latency Benchmark ===")
	t.Logf("CPUs: %d, Workers: %d, Keys: %d, Ops/Worker: %d", numCPU, workers, keys, ops)

	//goland:noinspection GoBoolExpressions
	t.Logf("Warmup: %d rounds, Measure: %d rounds, useBuiltInHasher: %v\n",
		warmupRounds, measureRounds, useBuiltInHasher)

	impls := []struct {
		name string
		make func() MapInterface
	}{
		{"sync.Map", func() MapInterface { return &syncMapAdapter{&sync.Map{}} }},
		{"RWShardedMap", func() MapInterface { return &rwShardedMapAdapter{NewRWLockShardedMap[int, int](numCPU * 4)} }},
		{"xsync.Map", func() MapInterface { return &xsyncMapAdapter{xsync.NewMap[int, int]()} }},
		{"synx.Map", func() MapInterface { return &mapAdapter{newMap()} }},
		{"synx.FlatMap", func() MapInterface { return &flatMapAdapter{newFlatMap()} }},
	}

	var results []*latencyResult
	for _, impl := range impls {
		t.Logf("Testing %s...", impl.name)
		r := runWithWarmup(workers, keys, ops, impl.make)
		r.name = impl.name
		results = append(results, &r)
	}

	// Sort by slowRate (lower is better)
	slices.SortFunc(results, func(a, b *latencyResult) int {
		if a.slowRate < b.slowRate {
			return -1
		}
		if a.slowRate > b.slowRate {
			return 1
		}
		return 0
	})

	t.Log("\n=== Results (sorted by p999) ===")
	t.Logf("%-4s | %-16s | %12s | %10s | %10s | %10s | %8s",
		"Rank", "Implementation", "Throughput", "p99", "p999", "max", "slow%")
	t.Logf("-----|------------------|--------------|------------|------------|------------|----------")
	for i, r := range results {
		t.Logf("%-4d | %-16s | %10.0f/s | %10s | %10s | %10s | %6.4f%%",
			i+1, r.name, r.throughput, ms(r.p99), ms(r.p999), ms(r.max), r.slowRate)
	}

	t.Logf("\n✓ Best p999: %s (%s)", results[0].name, ms(results[0].p999))

	// Find best throughput
	best := results[0]
	for _, r := range results {
		if r.throughput > best.throughput {
			best = r
		}
	}
	t.Logf("✓ Best throughput: %s (%.0f/s)", best.name, best.throughput)
}

// ============================================================================
// Quick Test (for fast iteration)
// ============================================================================

func TestLatencyQuick(t *testing.T) {
	numCPU := runtime.GOMAXPROCS(0)
	workers := numCPU
	keys := 100
	ops := 10000

	t.Logf("Quick test: %d workers, %d keys, %d ops", workers, keys, ops)

	impls := []struct {
		name string
		m    MapInterface
	}{
		{"sync.Map", &syncMapAdapter{&sync.Map{}}},
		{"RWShardedMap", &rwShardedMapAdapter{NewRWLockShardedMap[int, int](numCPU * 4)}},
		{"xsync.Map", &xsyncMapAdapter{xsync.NewMap[int, int]()}},
		{"synx.Map", &mapAdapter{newMap()}},
		{"synx.FlatMap", &flatMapAdapter{newFlatMap()}},
	}

	t.Logf("%-16s | %12s | %10s | %10s",
		"Implementation", "Throughput", "p999", "max")
	t.Logf("-----------------|--------------|------------|------------")

	for _, impl := range impls {
		r := runLatencyTest(workers, keys, ops, impl.m)
		t.Logf("%-16s | %10.0f/s | %10s | %10s",
			impl.name, r.throughput, ms(r.p999), ms(r.max))
	}
}
